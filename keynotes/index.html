---
title: SBFT'23 Keynotes
layout: default
---
<div class="col-md-8 ml-auto mr-auto text-left">

  <div class="section section-team text-left">
    <div class="container">
      <h1 class="title">Keynote Talks</h1>
      <div class="team">
        <div class="row">
          <div class="team-player">
            <img src="/img/briand.jpg" alt="Thumbnail Image"
              class="rounded-circle img-fluid img-raised profile-picture">
            <h4 class="title">Lionel Briand</h4>
            <p class="text-secondary">Nanda Laboratory, EECS Department, University of Ottawa and the Software
              Verification and Validation Laboratory, Centre for ICT Security, Reliability, and Trust (SnT),
              University of Luxembourg</p>
            <h5 class="text-primary">Revisiting the Notion of Diversity in Software Testing</h5>
            <p><a href="https://www.slideshare.net/briand_lionel/revisiting-the-notion-of-diversity-in-software-testing" target="_blank" class="fa fa-person-chalkboard"> Slides</a></p>
            <p><iframe width="100%" height="315" src="https://www.youtube.com/embed/EF13eiidhA0?start=9932" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
            <p class="text-justify">
              <strong>Abstract</strong>:
              The notion of diversity has been used to automate various software testing tasks, for example test selection and minimisation. 
              Intuitively, it is clear that more diverse test inputs and outputs are likely to detect more faults by more extensively exercising the software under test. 
              However, measuring diversity usually comes at a significant cost, alternatives based on different information sources need to be considered, and trade-offs are required.
              The way diversity is measured therefore varies significantly depending on the context of application and scalability considerations.
              This presentation will reflect on many years of experience during which that concept has been used in test automation, across various application contexts, to help devise practical and scalable testing solutions. 
            </p>
            <p class="text-justify">
              <strong>Biography</strong>: Lionel C. Briand is professor of software engineering and has shared
              appointments between (1) The University of Ottawa, Canada and (2) The SnT centre for Security,
              Reliability, and Trust, University of Luxembourg. In collaboration with colleagues, over 25 years, he
              has run many collaborative research projects with companies in the automotive, satellite, aeropsace,
              energy, financial, and legal domains. Lionel has held various engineering, academic, and leading
              positions in six countries. He was one of the founders of the ICST conference (IEEE Int. Conf. on
              Software Testing, Verification, and Validation, a CORE A event) and its first general chair. He was also
              EiC of Empirical Software Engineering (Springer) for 13 years and led, in collaboration with first
              Victor Basili and then Tom Zimmermann, the journal to the top tier of the very best publication venues
              in software engineering.
              <br />
              Lionel was elevated to the grades of IEEE Fellow and ACM Fellow for his work on software testing and
              verification. He was granted the IEEE Computer Society Harlan Mills award, the ACM SIGSOFT outstanding
              research award, and the IEEE Reliability Society engineer-of-the-year award, respectively in 2012, 2022,
              and 2013. He received an ERC Advanced grant in 2016 — on the topic of modelling and testing
              cyber-physical systems — which is the most prestigious individual research award in the European Union.
              He currently holds a Canada Research Chair (Tier 1) on "Intelligent Software Dependability and
              Compliance". His research interests include: software testing and verification, applications of AI in
              software engineering, model-driven software development, requirements engineering, and empirical
              software engineering.
            </p>
          </div>
        </div>
        <div class="row">
          <div class="team-player">
            <img src="/img/jane.jpg" alt="Thumbnail Image"
              class="rounded-circle img-fluid img-raised profile-picture">
            <h4 class="title">Jane Cleland-Huang</h4>
            <p class="text-secondary">Department of Computer Science and Engineering at the University of Notre
              Dame.</p>
            <h5 class="text-primary">Truth or Dare: Real-World Fuzz Testing of UAVs in
              Flight</h5>
            <p><iframe width="100%" height="315" src="https://www.youtube.com/embed/EF13eiidhA0?start=571" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
            <p class="text-justify">
              <strong>Abstract</strong>:
              Small Unmanned Aerial Systems (sUAS) need to meet rigorous safety requirements when deployed in
              high-stress emergency response scenarios. This requires systematic testing of the hardware, software,
              communication networks, and human interaction points. Fuzz testing is particularly effective at finding
              failures in edge cases that might otherwise be missed. In Cyber-Physical Systems, such as sUAS, tests
              typically move from simulation to the real-world; however, tests which execute perfectly in simulation
              can sometimes fail dramatically when run on physical sUAS, highlighting the importance of augmenting
              simulations with real-world Fuzz tests. Unfortunately real-world fuzzing is not only incredibly
              time-consuming, but also potentially hazardous because the inevitable failures produced by effective
              fuzz tests can cause sUAS to physically crash or fly-away. In this talk, Cleland-Huang will draw upon
              her own real-world experiences of developing and validating software for sUAS applications, and will
              explore the `truths’ and ‘dares’ of Fuzzing in the Field. The truth is that in the normal course of
              running physical flight tests, accidental erroneous input values can cause dramatic failures and
              crashes, but can also be incredibly helpful at revealing points of fragility in the system where
              modifications are needed. However, accidental fuzzing falls far short of the systematic goals of true
              fuzzing, introducing the non-trivial dilemma of how to safely deploy Fuzz Testing in the field. This
              talk explores this `dare’, by exploring a systematic approach for fuzz testing physical sUAS systems,
              thereby empowering testers to identify real-world weaknesses in edge-cases that could have been missed
              in simulation. The end result is increased robustness in real-world sUAS systems.
            </p>
            <p class="text-justify">
              <strong>Biography</strong>: Jane Cleland-Huang is the Frank M. Freimann Professor of Computer Science
              Chair and Department Chair of Computer Science and Engineering at the University of Notre Dame. Her
              research interests focus on Requirements Engineering, Software and Systems Traceability, and Safety
              Assurance for Cyber-Physical Systems (CPS). She is the Project Lead on the DroneResponse project which
              was initially developed as a research platform for supporting Software Engineering research in
              multi-agent CPS, but is now the core platform for a commercial system for deploying small Unmanned
              Aerial Systems (sUAS) in Emergency Response scenarios. Jane has served as Program Chair for several
              conferences including the IEEE Requirements Engineering Conference (2010), ESEC/FSE (2014), ICSE (2020),
              CAIN (2022), and SPLC (2022). She has previously served as Associate Editor of IEEE Transactions on
              Software Engineering and on the IEEE Software editorial board, and currently serves as Chair of IFIP 2.9
              Working group on Requirements Engineering, and on the Editorial Boards for Communications of the ACM abd
              Springer Verlag Requirements Engineering journal.

              <br />

              Along with members of her research group she has been the recipient of seven ACM SIGSOFT Distinguished
              Paper awards, and the Mannfred Paul award for Excellence in Software Theory and Practice. Jane is
              committed to supporting a diverse, equitable, and inclusive community of Software Engineering
              researchers and envisions a future in which our research community is truly reflective of the population
              around us. She is also passionate about impacting the world in a positive way through technology
              transfer that takes research into practice, and as a result is currently engaged in two spin-off
              companies – DroneResponse and SAFA.”
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>